# Smart RAG Agent with Gradio

A memory-based Retrieval-Augmented Generation (RAG) pipeline with a **Gradio** interface. Upload documents, ingest them into a vector database, and chat with context-aware AI responses. The system uses **LangChain**, **FAISS**, and **Groq LLMs**, with persistent chat history for each user session.

---

## ğŸš€ Features

* **Document ingestion**: Upload PDFs, DOCX, images (OCR with Tesseract), and more.
* **Vector embeddings**: Powered by `sentence-transformers` via HuggingFace.
* **Retrieval-Augmented Generation (RAG)**: Context-aware answers using FAISS as a vector store.
* **Chat with memory**: Saves and reloads conversations per session.
* **Gradio UI**: Clean and interactive chat interface.
* **Dockerized**: Fully containerized with support for Tesseract and Poppler.

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ app.py                # Main Gradio application entrypoint
â”œâ”€â”€ embedding/             # Handles document Document Ingestion , creating vector embedding and a retriever
â”œâ”€â”€ helpers/               # Handles LLM initiation , prompt definition and setting up pydantic Schemas 
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ rag/                  # Directory for rag implementation
â”œâ”€â”€ settings.py/          # Defines the necessary environment variables and constants required
â””â”€â”€ README.md             # Documentation (this file)
```

---

## âš™ï¸ Installation

### Prerequisites

* Python 3.11+
* [Tesseract OCR](https://github.com/tesseract-ocr/tesseract)
* [Poppler](https://poppler.freedesktop.org/) (for PDF â†’ image conversion)

### Clone the repo

```bash
git clone https://github.com/<your-username>/<your-repo>.git
cd <your-repo>
```

### Setup environment

```bash
python -m venv venv
source venv/bin/activate   # Linux / macOS
venv\Scripts\activate      # Windows

pip install --upgrade pip
pip install -r requirements.txt
```

### Run locally

```bash
python app.py
```

Visit **[http://localhost:8000](http://localhost:8000)** in your browser.

---



If using **Gradio**, make sure `app.py` launches with:

```python
demo.launch(server_name="0.0.0.0", server_port=int(os.getenv("PORT", 8000)))
```

---

## ğŸ“– Usage

1. Upload a document via the **Upload Document** button.
2. Ask a question in the chat box.
3. The AI retrieves relevant document chunks and responds contextually.
4. The responses are context aware and the last chat history are stored in memory

---

## ğŸ§° Tech Stack

* [LangChain](https://www.langchain.com/) â€“ Orchestration
* [Groq LLMs](https://groq.com/) â€“ Fast inference
* [FAISS](https://faiss.ai/) â€“ Vector database
* [HuggingFace Transformers](https://huggingface.co/) â€“ Embeddings
* [Gradio](https://www.gradio.app/) â€“ User interface
* [Tesseract OCR](https://github.com/tesseract-ocr/tesseract) â€“ OCR for image ingestion
* [Poppler](https://poppler.freedesktop.org/) â€“ PDF image conversion


---

## ğŸ”® Future Improvements

* Support for multiple documents per session.
* Cloud-based vector DB integration (Pinecone, Weaviate, ChromaDB).
* User authentication with session-based history storage.
* Advanced UI with filtering, highlighting, and file management.
* Fine-tuning embeddings for domain-specific corpora.

---

## ğŸ¤ Contributing

Contributions are welcome! Please open an issue or PR.



MIT License. See `LICENSE` file for details.
